{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    shortcut is to use pandas_profiling, sweetviz, and/or autoviz packages."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas_profiling (ydata_profiling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ydata_profiling\n",
    "\n",
    "#profile = ydata_profiling.ProfileReport(df)\n",
    "\n",
    "#profile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sweetviz as sv\n",
    "\n",
    "#report = sv.analyze(df)\n",
    "\n",
    "#report.show_notebook()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autoviz (CSV, txt, or JSON formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from autoviz.AutoViz_Class import AutoViz_Class\n",
    "\n",
    "#AV = AutoViz_Class()\n",
    "\n",
    "# dft = AV.AutoViz(filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual data characteristics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "\n",
    "def get_df_characteristics(df):\n",
    "\n",
    "    # dimensions, col names, presence of NaNs, data types\n",
    "    print(df.info())\n",
    "\n",
    "    # Missing values\n",
    "    missing_values = df.isnull().sum().reset_index()\n",
    "    missing_values.columns = ['Column', 'Missing Values']\n",
    "\n",
    "    # Summary statistics\n",
    "    summary_stats = df.describe().reset_index()\n",
    "    summary_stats.columns = ['Statistic', 'Column'] + summary_stats.columns[2:].tolist()\n",
    "\n",
    "    # Unique values count for each column\n",
    "    unique_values = df.nunique().reset_index()\n",
    "    unique_values.columns = ['Column', 'No. Unique Values']\n",
    "\n",
    "    # Print the characteristics using tabulate\n",
    "    print()\n",
    "    print(\"Missing values:\")\n",
    "    print(tabulate(missing_values, headers='keys', tablefmt='grid', showindex=False))\n",
    "    print()\n",
    "    print(\"Summary statistics:\")\n",
    "    print(tabulate(summary_stats, headers='keys', tablefmt='grid', showindex=False))\n",
    "    print()\n",
    "    print(\"No. unique values:\")\n",
    "    print(tabulate(unique_values, headers='keys', tablefmt='grid', showindex=False))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_feature_distributions(df):\n",
    "    num_plots = len(df.columns)\n",
    "    num_rows = 2\n",
    "    num_cols = num_plots // num_rows + num_plots % num_rows\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 10))\n",
    "    fig.tight_layout(pad=4.0)\n",
    "\n",
    "    for i, column in enumerate(df.columns):\n",
    "        ax = axes[i // num_cols, i % num_cols]\n",
    "        if df[column].dtype.kind in 'biufc': #  'b' for boolean, 'i' for signed integer, 'u' for unsigned integer, 'f' for floating-point, and 'c' for complex.\n",
    "            sns.histplot(data=df, x=column, ax=ax) # if numeric\n",
    "        else:\n",
    "            sns.countplot(data=df, x=column, ax=ax) # if non-numeric\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot correlations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Pearson Correlation: Default. Measures the linear relationship between two variables and is appropriate for continuous variables. Specify by setting correlation_type='pearson'.\n",
    "\n",
    "    Spearman Correlation: Measures the monotonic relationship between two variables, which captures both linear and non-linear associations. Suitable for continuous or ordinal variables. Specify by setting correlation_type='spearman'.\n",
    "\n",
    "    Kendall Correlation: Measures the strength of the ordinal association between two variables. Useful for assessing the relationship between non-normally distributed ordinal variables. Specify by setting correlation_type='kendall'.\n",
    "\n",
    "    Point Biserial Correlation: Measures the association between a continuous variable and a binary variable. Special case of the Pearson correlation. Specify by setting correlation_type='pointbiserial'.\n",
    "\n",
    "    Cramer's V: Measures the association between two categorical variables and is suitable for nominal variables. Based on chi-square statistics. Specify setting correlation_type='cramers'.\n",
    "\n",
    "    Phi Coefficient: Measures the association between two binary variables. Similar to point biserial correlation. You can specify it by setting correlation_type='phi'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_correlation(df, correlation_type='pearson'):\n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df.corr(method=correlation_type)\n",
    "\n",
    "    # Set up the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(20, 16))\n",
    "\n",
    "    # Create a mask for the upper triangle\n",
    "    mask = ~corr_matrix.isnull()\n",
    "\n",
    "    # Plot the correlation matrix heatmap\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True, square=True, ax=ax)\n",
    "\n",
    "    # Customize the plot\n",
    "    ax.set_title(f\"{correlation_type.capitalize()} Correlation Matrix\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan(df, method, columns=None):\n",
    "    if columns is None:\n",
    "        columns = df.columns  # If columns are not specified, fill NaN values for all columns\n",
    "    \n",
    "    if method == \"mean\":\n",
    "        filled_df = df.copy()\n",
    "        filled_df[columns] = filled_df[columns].fillna(filled_df[columns].mean())\n",
    "    elif method == \"median\":\n",
    "        filled_df = df.copy()\n",
    "        filled_df[columns] = filled_df[columns].fillna(filled_df[columns].median())\n",
    "    elif method == \"mode\":\n",
    "        filled_df = df.copy()\n",
    "        for column in columns:\n",
    "            filled_df[column] = filled_df[column].fillna(filled_df[column].mode().iloc[0])\n",
    "    elif method == \"zero\":\n",
    "        filled_df = df.copy()\n",
    "        filled_df[columns] = filled_df[columns].fillna(0)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid imputation method. Please choose 'mean', 'median', 'mode', or 'zero'.\")\n",
    "    \n",
    "    return filled_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X, y, train test split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Specify dataframe, and target column as predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def process_df(df, target_column, test_size, random_state, stratify_var):\n",
    "    # Splitting into features and target variable\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Perform train-test split with specified parameters\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=stratify_var)\n",
    "    \n",
    "    return X, y, X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding entire df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def perform_OHE_df(df):\n",
    "    # Identify categorical columns\n",
    "    categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "    \n",
    "    # Perform one-hot encoding\n",
    "    ohe = OneHotEncoder(sparse=False, drop='first')\n",
    "    transformed = ohe.fit_transform(df[categorical_cols])\n",
    "    \n",
    "    # Create new DataFrame with transformed features\n",
    "    transformed_df = pd.DataFrame(transformed, columns=ohe.get_feature_names_out(categorical_cols))\n",
    "    \n",
    "    # Combine transformed features with non-categorical columns\n",
    "    df_transformed = pd.concat([df.drop(categorical_cols, axis=1), transformed_df], axis=1)\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "\n",
    "# example usage\n",
    "# df_transformed = perform_OHE_df(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_OHE_feature(df, features):\n",
    "    # Perform one-hot encoding\n",
    "    ohe = OneHotEncoder(sparse=False, drop='first')\n",
    "    transformed = ohe.fit_transform(df[features])\n",
    "    \n",
    "    # Create new DataFrame with transformed features\n",
    "    transformed_df = pd.DataFrame(transformed, columns=ohe.get_feature_names_out(features))\n",
    "    \n",
    "    # Drop original columns and combine transformed features with the DataFrame\n",
    "    df_transformed = df.drop(features, axis=1)\n",
    "    df_transformed = pd.concat([df_transformed, transformed_df], axis=1)\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# features = ['column1', 'column2'] # specify columns to OHE\n",
    "# df_transformed = perform_OHE_feature(df, features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def ordinal_encode(df, columns):\n",
    "    encoder = OrdinalEncoder()\n",
    "    df[columns] = encoder.fit_transform(df[columns])\n",
    "    return df\n",
    "\n",
    "# example usage\n",
    "# columns_to_encode = ['column1', 'column2']  # Specify the categorical columns to encode\n",
    "# df_encoded = ordinal_encode(df, columns_to_encode)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encode(df, columns):\n",
    "    encoder = LabelEncoder()\n",
    "    df[columns] = df[columns].apply(encoder.fit_transform)\n",
    "    return df\n",
    "\n",
    "\n",
    "# example usage\n",
    "#columns_to_encode = ['column1', 'column2']  # Specify the categorical columns to encode\n",
    "#df_encoded = label_encode(df, columns_to_encode)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standardisation/normalisation/feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def scale_features(df, numerical_features, scaling_method):\n",
    "    if scaling_method == \"standardisation\":\n",
    "        scaler = StandardScaler()\n",
    "        df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "    elif scaling_method == \"normalisation\":\n",
    "        scaler = MinMaxScaler()\n",
    "        df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "    elif scaling_method == \"-1\":\n",
    "        # Specify your custom range for scaling\n",
    "        custom_min = -1\n",
    "        custom_max = 1\n",
    "        scaled_range = custom_max - custom_min\n",
    "\n",
    "        min_val = df[numerical_features].min()\n",
    "        max_val = df[numerical_features].max()\n",
    "        df[numerical_features] = custom_min + ((df[numerical_features] - min_val) / (max_val - min_val)) * scaled_range\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scaling method. Please choose 'standardisation', 'normalisation', or '-1'.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# example usage\n",
    "# numerical_features = ['feature1', 'feature2', 'feature3']  # Specify the numerical features to scale\n",
    "# scaling_method = \"standardisation\"  # Specify the scaling method (\"standardisation\", \"normalisation\", or \"-1\")\n",
    "\n",
    "#df_scaled = scale_features(df, numerical_features, scaling_method)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detect outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_outliers(df, numerical_features, threshold=3):\n",
    "    z_scores = np.abs((df[numerical_features] - df[numerical_features].mean()) / df[numerical_features].std())\n",
    "    outlier_mask = z_scores > threshold\n",
    "    return outlier_mask.any(axis=1)\n",
    "\n",
    "# example usage\n",
    "# numerical_features = ['feature1', 'feature2', 'feature3']  # Specify the numerical features for outlier detection\n",
    "# outlier_threshold = 3  # Specify the threshold for the Z-score method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handle outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(df, outlier_mask, strategy=\"remove\", value=None):\n",
    "    if strategy == \"remove\":\n",
    "        df_cleaned = df[~outlier_mask]\n",
    "    elif strategy == \"impute\" and value is not None:\n",
    "        df_cleaned = df.copy()\n",
    "        df_cleaned.loc[outlier_mask] = value\n",
    "    else:\n",
    "        raise ValueError(\"Invalid outlier handling strategy or missing value for imputation.\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# example usage\n",
    "# outlier_mask = detect_outliers(df, numerical_features, threshold=outlier_threshold)\n",
    "#\n",
    "## Remove outliers\n",
    "# df_cleaned = handle_outliers(df, outlier_mask, strategy=\"remove\")\n",
    "#\n",
    "## Impute outliers with a specific value\n",
    "# df_imputed = handle_outliers(df, outlier_mask, strategy=\"impute\", value=999)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def perform_recursive_feature_elimination(X, y, n_features):\n",
    "    estimator = LogisticRegression(solver='liblinear')\n",
    "    selector = RFE(estimator, n_features_to_select=n_features)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.support_]\n",
    "    return X_new, selected_features\n",
    "\n",
    "\n",
    "# example usage\n",
    "\n",
    "#X = ...  # Your feature matrix\n",
    "#y = ...  # Your target variable\n",
    "\n",
    "#n_features = 5  # Number of features to select\n",
    "#X_new, selected_features = perform_recursive_feature_elimination(X, y, n_features)\n",
    "#print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histogram(df, feature):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(df[feature].dropna(), kde=True)\n",
    "    plt.title(f\"Histogram of {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlation_matrix(df):\n",
    "    corr_matrix = df.corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(\"Correlation Matrix\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
